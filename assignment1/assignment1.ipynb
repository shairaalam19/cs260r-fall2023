{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Tabular Reinforcement Learning\n",
    "\n",
    "-----\n",
    "\n",
    "*CS260R 2023Fall: Reinforcement Learning. Department of Computer Science at University of California, Los Angeles. Course Instructor: Professor Bolei ZHOU. Assignment author: Zhenghao PENG.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    Student Name     |        Student ID         |\n",
    "|:-------------------:|:-------------------------:|\n",
    "| Shaira Alam | 506302126 |\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the assignment 1 of our reinforcement learning course. The objective of this assignment is for you to understand the classic methods used in tabular RL. \n",
    "\n",
    "This assignment has the following sections:\n",
    "\n",
    " - Section 1: Warm-up on the RL environment (35 points)\n",
    " - Section 2: Implementation of the model-based family of algorithms: policy iteration and value iteration. (65 points)\n",
    "\n",
    "You need to go through this self-contained notebook, with dozens of **TODO**s are scattered in the cells. You need to finish all TODOs.\n",
    "\n",
    "You are encouraged to add more code on extra cells at the end of each section to investigate the problems you think interesting. At the end of the file, we leave a place for you to write comments optionally (Yes, please give us either negative or positive rewards so that we can keep improving the assignment!).\n",
    "\n",
    "Please report any code bugs to us via [**GitHub issues**](https://github.com/ucla-rlcourse/assignment-2022fall).\n",
    "\n",
    "Before you get start, remember to follow the instruction at https://github.com/ucla-rlcourse/assignment-2022fall/tree/main/assignment0\n",
    "to set up your python environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "\n",
    "This assignment requires the following dependencies:\n",
    "\n",
    "1. `gymnasium==0.29.1`\n",
    "2. `numpy`\n",
    "3. `scipy`\n",
    "\n",
    "You can install all of them through the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (23.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapy in c:\\python310\\lib\\site-packages (1.1.9)\n",
      "Requirement already satisfied: numpy in c:\\python310\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\python310\\lib\\site-packages (1.9.3)\n",
      "Collecting gymnasium==0.29.1\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\python310\\lib\\site-packages (from gymnasium==0.29.1) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\python310\\lib\\site-packages (from gymnasium==0.29.1) (4.4.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\python310\\lib\\site-packages (from gymnasium==0.29.1) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in c:\\python310\\lib\\site-packages (from gymnasium[toy-text]==0.29.1) (2.5.2)\n",
      "Requirement already satisfied: ipython in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from mediapy) (8.7.0)\n",
      "Requirement already satisfied: matplotlib in c:\\python310\\lib\\site-packages (from mediapy) (3.7.1)\n",
      "Requirement already satisfied: Pillow in c:\\python310\\lib\\site-packages (from mediapy) (10.0.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->mediapy) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\python310\\lib\\site-packages (from ipython->mediapy) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->mediapy) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->mediapy) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->mediapy) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.11 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->mediapy) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->mediapy) (2.13.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->mediapy) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->mediapy) (5.6.0)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from ipython->mediapy) (0.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python310\\lib\\site-packages (from matplotlib->mediapy) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python310\\lib\\site-packages (from matplotlib->mediapy) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python310\\lib\\site-packages (from matplotlib->mediapy) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python310\\lib\\site-packages (from matplotlib->mediapy) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->mediapy) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->mediapy) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python310\\lib\\site-packages (from matplotlib->mediapy) (2.8.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython->mediapy) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from prompt-toolkit<3.1.0,>=3.0.11->ipython->mediapy) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapy) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->mediapy) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->mediapy) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->mediapy) (0.2.2)\n",
      "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "   ---------------------------------------- 953.9/953.9 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: gymnasium\n",
      "  Attempting uninstall: gymnasium\n",
      "    Found existing installation: gymnasium 0.28.1\n",
      "    Uninstalling gymnasium-0.28.1:\n",
      "      Successfully uninstalled gymnasium-0.28.1\n",
      "Successfully installed gymnasium-0.29.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "metadrive-simulator 0.4.1.2 requires gymnasium<0.29,>=0.28, but you have gymnasium 0.29.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# If you already installed everything, you don't need to run this cell.\n",
    "# Install dependencies to your current python environment.\n",
    "\n",
    "!pip install -U pip\n",
    "!pip install mediapy numpy scipy \"gymnasium==0.29.1\" \"gymnasium[toy-text]==0.29.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start running the cells sequentially (by `ctrl + enter` or `shift + enter`) to avoid unnecessary errors by skipping some cells. \n",
    "\n",
    "\n",
    "## Section 1: Warm-up on the RL environment\n",
    "\n",
    "(35/100 points)\n",
    "\n",
    "In this section, we will go through the basic concepts of RL environments using OpenAI Gym. Besides, you will get the first sense of the toy environment we will use in the rest of the assignment.\n",
    "\n",
    "Every Gym environment should contain the following attributes:\n",
    "\n",
    "1. `env.step(action)` To advance the environment by one time step through applying `action`. Will return four things: `observation, reward, terminated, truncated, info`, wherein `terminated` is a boolean value indicating whether this **episode** is finished either by the agent successfully finishes the task or makes something wrong so the episode is not valid (like the agent dies), `truncated` is a boolean value indicating whether this episode reach the maximum step limit. We sometime use `done = terminated or truncated` as an indicator that an episode is ended. `info` is a dict containing some information the user is interested in.\n",
    "2. `env.reset()` To reset the environment, back to the initial state. Will return the initial observation of the new episode.\n",
    "3. `env.render()` To render the current state of the environment for human-being\n",
    "4. `env.action_space` The allowed action format. In our case, it is `Discrete(4)` which means the action is an integer in the range [0, 1, 2, 3]. Therefore, the `action` for `step(action)` should obey the limit of the action space.\n",
    "5. `env.observation_space` The observation space.\n",
    "\n",
    "\n",
    "Note that the word **episode** means the process that an agent interacts with the environment from the initial state to the terminal state. Within one episode, the agent will only receive one `done=True`, when it goes to the terminal state (the agent is dead or the game is over).\n",
    "\n",
    "We will use `FrozenLake8x8-v1` as our environment. In this environment, the agent controls the movement of a *character* in a grid world. Some tiles of the grid are walkable, and others are not, making to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. The meaning of each character:\n",
    "\n",
    "1. S : starting point, safe\n",
    "2. F : frozen surface, safe\n",
    "3. H : hole, fall to your doom\n",
    "4. G : goal, where the frisbee is located\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Documents\\School\\2023 Fall\\CS260R\\cs260r-assignment-2023fall\\assignment1\\assignment1.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/School/2023%20Fall/CS260R/cs260r-assignment-2023fall/assignment1/assignment1.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List, Callable\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/School/2023%20Fall/CS260R/cs260r-assignment-2023fall/assignment1/assignment1.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Import some packages that we need to use\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/School/2023%20Fall/CS260R/cs260r-assignment-2023fall/assignment1/assignment1.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgym\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/School/2023%20Fall/CS260R/cs260r-assignment-2023fall/assignment1/assignment1.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Documents/School/2023%20Fall/CS260R/cs260r-assignment-2023fall/assignment1/assignment1.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Prepare some useful functions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\gymnasium\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"Root `__init__` of the gymnasium module setting the `__all__` of gymnasium modules.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# isort: skip_file\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     Env,\n\u001b[0;32m      6\u001b[0m     Wrapper,\n\u001b[0;32m      7\u001b[0m     ObservationWrapper,\n\u001b[0;32m      8\u001b[0m     ActionWrapper,\n\u001b[0;32m      9\u001b[0m     RewardWrapper,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspaces\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspace\u001b[39;00m \u001b[39mimport\u001b[39;00m Space\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistration\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     make,\n\u001b[0;32m     14\u001b[0m     spec,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     register_envs,\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\gymnasium\\core.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcopy\u001b[39;00m \u001b[39mimport\u001b[39;00m deepcopy\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m TYPE_CHECKING, Any, Generic, SupportsFloat, TypeVar\n\u001b[1;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m \u001b[39mimport\u001b[39;00m logger, spaces\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m RecordConstructorArgs, seeding\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\__init__.py:143\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n\u001b[1;32m--> 143\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m lib\n\u001b[0;32m    144\u001b[0m \u001b[39m# NOTE: to be revisited following future namespace cleanup.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[39m# See gh-14454 and gh-15672 for discussion.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\lib\\__init__.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marraysetops\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnpyio\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marrayterator\u001b[39;00m \u001b[39mimport\u001b[39;00m Arrayterator\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marraypad\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m Mapping\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mformat\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_datasource\u001b[39;00m \u001b[39mimport\u001b[39;00m DataSource\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m overrides\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "import time\n",
    "from typing import List, Callable\n",
    "\n",
    "# Import some packages that we need to use\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "# Prepare some useful functions\n",
    "from IPython.display import clear_output\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def wait(sleep=0.2):\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(sleep)\n",
    "\n",
    "\n",
    "def print_table(data):\n",
    "    if data.ndim == 2:\n",
    "        for i in range(data.shape[1]):\n",
    "            print(\"\\n=== The state value for action {} ===\".format(i))\n",
    "            print_table(data[:, i])\n",
    "        return\n",
    "    assert data.ndim == 1, data\n",
    "    if data.shape[0] == 16:  # FrozenLake-v0\n",
    "        text = \"+-----+-----+-----+-----+-----+\\n\" \\\n",
    "               \"|     |   0 |   1 |   2 |   3 |\\n\" \\\n",
    "               \"|-----+-----+-----+-----+-----+\\n\"\n",
    "        for row in range(4):\n",
    "            tmp = \"| {}   |{:.3f}|{:.3f}|{:.3f}|{:.3f}|\\n\" \\\n",
    "                  \"|     |     |     |     |     |\\n\" \\\n",
    "                  \"+-----+-----+-----+-----+-----+\\n\" \\\n",
    "                  \"\".format(\n",
    "                row, *[data[row * 4 + col] for col in range(4)]\n",
    "            )\n",
    "            text = text + tmp\n",
    "    else:\n",
    "        text = \"+-----+-----+-----State Value Mapping-----+-----+-----+\\n\" \\\n",
    "               \"|     |   0 |   1 |   2 |   3 |   4 |   5 |   6 |   7 |\\n\" \\\n",
    "               \"|-----+-----+-----+-----+-----+-----+-----+-----+-----|\\n\"\n",
    "        for row in range(8):\n",
    "            tmp = \"| {}   |{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{:.3f}|{\" \\\n",
    "                  \":.3f}|\\n\" \\\n",
    "                  \"|     |     |     |     |     |     |     |     |     |\\n\" \\\n",
    "                  \"+-----+-----+-----+-----+-----+-----+-----+-----+-----+\\n\" \\\n",
    "                  \"\".format(\n",
    "                row, *[data[row * 8 + col] for col in range(8)]\n",
    "            )\n",
    "            text = text + tmp\n",
    "    print(text)\n",
    "\n",
    "\n",
    "def test_random_policy(policy, env):\n",
    "    _acts = set()\n",
    "    for i in range(1000):\n",
    "        act = policy(0)\n",
    "        _acts.add(act)\n",
    "        assert env.action_space.contains(act), \"Out of the bound!\"\n",
    "    if len(_acts) != 1:\n",
    "        print(\n",
    "            \"[HINT] Though we call self.policy 'random policy', \"\n",
    "            \"we find that generating action randomly at the beginning \"\n",
    "            \"and then fixing it during updating values period lead to better \"\n",
    "            \"performance. Using a stochastic policy is not even work! \"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1: Make the environment\n",
    "\n",
    "You need to know \n",
    "\n",
    "1. How to make an environment\n",
    "2. How to set the random seed of environment\n",
    "3. What is observation space and action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# TODO: Just a reminder. Do you add your name and student \n",
    "# ID in the table at top of the notebook?\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('FrozenLake8x8-v1', render_mode=\"ansi\")\n",
    "\n",
    "# You need to reset the environment immediately after instantiating env. \n",
    "env.reset(seed=0)  # TODO: uncomment this line\n",
    "\n",
    "print(\"Current observation space: {}\".format(env.observation_space))\n",
    "print(\"Current action space: {}\".format(env.action_space))\n",
    "print(\"0 in action space? {}\".format(env.action_space.contains(0)))\n",
    "print(\"5 in action space? {}\".format(env.action_space.contains(5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification to get a sense of the environment.\n",
    "tmp_env = gym.make('FrozenLake8x8-v1', render_mode=\"rgb_array\")\n",
    "tmp_env.reset()\n",
    "_ = plt.imshow(tmp_env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2: Play the environment with random actions\n",
    "\n",
    "You need to know \n",
    "\n",
    "1. How to step the environment;\n",
    "2. How to rollout a complete episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Run 1000 steps for test, terminate if done.\n",
    "# You can run this cell multiples times.\n",
    "env.reset(seed=0)\n",
    "\n",
    "while True:\n",
    "    # Take random action\n",
    "    # TODO: Uncomment next two lines\n",
    "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # Render the environment.\n",
    "    # You will see the visualization of the behaviors of the agent\n",
    "    # if you are using local machine to run this notebook.\n",
    "    print(env.render())\n",
    "\n",
    "    print(\"Current observation: {}\\nCurrent reward: {}\\n\"\n",
    "          \"Whether we are done: {}\\ninfo: {}\".format(\n",
    "        observation, reward, done, info\n",
    "    ))\n",
    "\n",
    "    wait(sleep=0.1)\n",
    "\n",
    "    # TODO: Terminate the loop if done\n",
    "    if done: \n",
    "      break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3: Define the evaluation function to value the random baseline\n",
    "\n",
    "Now we need to define an evaluation function to evaluate a given policy.\n",
    "\n",
    "As a reminder, you should create a `FrozenLake8x8-v1` environment instance by default, reset it after each episode (and at the beginning), step the environment, and terminate the episode if done. According to Gym v26 update, \n",
    "\n",
    "After implementing the `evaluate` function, run the next cell to check whether the function is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "def _render_helper(env):\n",
    "    print(env.render())\n",
    "    wait(sleep=0.05)\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    policy: Callable,\n",
    "    num_episodes: int,\n",
    "    seed: int = 0,\n",
    "    env_name: str = 'FrozenLake8x8-v1',\n",
    "    render: bool = False,\n",
    "    render_mode: str = 'ansi',\n",
    ") -> float:\n",
    "    \"\"\"This function evaluates the given policy and returns the \n",
    "    average episodic return across #num_episodes episodes.\n",
    "    We use `seed` argument for testing purpose.\n",
    "    You should pass the tests in the next cell.\n",
    "\n",
    "    :param policy: a function whose input is an integer (observation)\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: an integer, used for testing.\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag. If true, please call _render_helper\n",
    "    function.\n",
    "    :param render_mode: a string specifies the render mode if render=True.\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create environment (according to env_name, we will use env other than 'FrozenLake8x8-v1')\n",
    "    env = gym.make(env_name, render_mode=render_mode if render else None)\n",
    "\n",
    "    # Build inner loop to run.\n",
    "    # For each episode, do not set the limit.\n",
    "    # Only terminate episode (reset environment) when done = True.\n",
    "    # The episode reward is the sum of all rewards happen within one episode.\n",
    "    # Call the helper function `render(env)` to render\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        # reset the environment\n",
    "        obs, info = env.reset(seed=seed + i)\n",
    "        action = policy(obs)\n",
    "\n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            # TODO: run the environment and terminate it if done, collect the\n",
    "            # reward at each step and sum them to the episode reward.\n",
    "            action = policy(obs)\n",
    "            \n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            ep_reward += reward\n",
    "            if render:\n",
    "                _render_helper(env)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "\n",
    "    return float(np.mean(rewards))\n",
    "\n",
    "# TODO: Run next cell to test your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Run this cell to test the correctness of your implementation of `evaluate`.\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "\n",
    "def expert(obs):\n",
    "    \"\"\"Go down if agent at the right edge, otherwise go right.\"\"\"\n",
    "    return DOWN if (obs + 1) % 8 == 0 else RIGHT\n",
    "\n",
    "\n",
    "def assert_equal(seed, value, env_name):\n",
    "    ret = evaluate(expert, 1000, seed, env_name=env_name)\n",
    "    assert ret == value, \\\n",
    "        \"When evaluate on seed {}~{} in {} environment, the \" \\\n",
    "        \"averaged reward should be {}. But you get {}.\" \\\n",
    "        \"\".format(seed, seed + 1000, env_name, value, ret)\n",
    "\n",
    "\n",
    "assert_equal(0, 0.046, 'FrozenLake8x8-v1')\n",
    "assert_equal(1000, 0.047, 'FrozenLake8x8-v1')\n",
    "assert_equal(2000, 0.065, 'FrozenLake8x8-v1')\n",
    "\n",
    "assert_equal(0, 0.024, 'FrozenLake-v1')\n",
    "assert_equal(1000, 0.034, 'FrozenLake-v1')\n",
    "assert_equal(2000, 0.035, 'FrozenLake-v1')\n",
    "\n",
    "print(\"Test Passed!\")\n",
    "print(\"\\nAs a baseline, the mean episode reward of a hand-craft \"\n",
    "      \"agent is: \", evaluate(expert, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation! You have finished section 1 (if and only if not error happens above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Section 2: Model-based Tabular RL\n",
    "\n",
    "(65/100 points)\n",
    "\n",
    "We have learned how to use the Gym environment to run an episode, as well as how to interact between the agent (policy) and environment via `env.step(action)` to collect observation, reward, done, and possible extra information.\n",
    "\n",
    "Now we need to build the basic tabular RL algorithm to solve this environment. **Note that compared to the model-free methods in the Sec.3, the algorithms in this section needs to access the internal information of the environment, namely the transition dynamics**. \n",
    "\n",
    "In our case, given a state and an action, we need to know which state current environment will jump to, the probability of this transition, and the reward of the transition. You will find that we provide you a helper function `self._get_transitions(state, action)` that takes state and action as input and return you a list of possible transitions.\n",
    "\n",
    "First, we will implement an abstract class to represent a Trainer. Though this seems to be over-complex for tabular RL, we will use the same framework in the future assignments. So it would be helpful for you to get familiar with how to implement an RL algorithm in the class-oriented programming style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TabularRLTrainerAbstract:\n",
    "    \"\"\"This is an abstract class for tabular RL trainer. We will subclass this class\n",
    "     to implement specific algorithm, so that we can reuse the codes like\n",
    "    getting the dynamic of the environment (self._get_transitions()) or rendering the\n",
    "    learned policy (self.render()).\"\"\"\n",
    "\n",
    "    def __init__(self, env_name='FrozenLake8x8-v1', model_based=True):\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(self.env_name)\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.n\n",
    "\n",
    "        self.model_based = model_based\n",
    "\n",
    "        # Define the policy as function that returns the selected action given a state.\n",
    "        self.policy = None\n",
    "\n",
    "        # Define the value table as a numpy array.\n",
    "        self.value_table = None\n",
    "\n",
    "    def _get_transitions(self, state: int, act: int) -> List:\n",
    "        \"\"\"Query the environment to get the transition probability,\n",
    "        reward, the next state, and done given a pair of state and action.\n",
    "        We implement this function for you. But you need to know the \n",
    "        return format of this function.\n",
    "        \"\"\"\n",
    "        self._check_env_name()\n",
    "        assert self.model_based, \"You should not use _get_transitions in \" \\\n",
    "                                 \"model-free algorithm!\"\n",
    "\n",
    "        # call the internal attribute of the environments.\n",
    "        # `transitions` is a list contain all possible next states and the \n",
    "        # probability, reward, and termination indicator corresponding to it\n",
    "        transitions = self.env.unwrapped.P[state][act]\n",
    "\n",
    "        # Given a state-action pair, it is possible\n",
    "        # to have multiple transitions, since the \n",
    "        # environment is not deterministic.\n",
    "        # The return of this function: a list of dicts\n",
    "        ret = []\n",
    "        for prob, next_state, reward, done in transitions:\n",
    "            ret.append({\n",
    "                \"prob\": prob,\n",
    "                \"next_state\": next_state,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done\n",
    "            })\n",
    "        return ret\n",
    "\n",
    "    def _check_env_name(self):\n",
    "        assert self.env_name.startswith('FrozenLake')\n",
    "\n",
    "    def print_table(self):\n",
    "        \"\"\"print beautiful table, only work for FrozenLake8X8-v1 env. We \n",
    "        write this function for you.\"\"\"\n",
    "        self._check_env_name()\n",
    "        print_table(self.value_table)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n",
    "\n",
    "    def evaluate(self, seed=1000):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 1000 episodes when seed=0.\"\"\"\n",
    "        result = evaluate(self.policy, seed=seed, num_episodes=1000, env_name=self.env_name)\n",
    "        return result\n",
    "\n",
    "    def render(self, seed=1000):\n",
    "        \"\"\"Reuse your evaluate function, render current policy \n",
    "        for one episode when seed=0\"\"\"\n",
    "        evaluate(self.policy, seed=seed, num_episodes=1, render=True, env_name=self.env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Run trainer._get_transitions and give you a sense of how it works.\n",
    "test_trainer = TabularRLTrainerAbstract()\n",
    "transitions = test_trainer._get_transitions(state=0, act=0)\n",
    "print(f\"The return transitions is a {type(transitions)}.\\n\\n{transitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1: Policy Iteration\n",
    "\n",
    "Recall the process of policy iteration: \n",
    "\n",
    "1. Update the state value function, given all possible transitions at current state of the environment.\n",
    "2. Find the best policy that earns the highest value under current state value function.\n",
    "3. If the best policy is identical to the previous one then stop the training. Otherwise, return to step 1.\n",
    "\n",
    "In step 1, update the state value function by\n",
    "\n",
    "$v_{k+1} = E_{s'}[r(s, a)+\\gamma v_{k}(s')]$\n",
    "\n",
    "wherein the $a$ is given by current policy, $s'$ is next state, $r$ is the reward, $v_{k}(s')$ is the next state value given by the old (not updated yet) value function.\n",
    "The expectation is computed among all possible transitions given a state and action pair (As the environment is not deterministic, it's possible to transit to different next states even given the same state-action pair).\n",
    "Note that the new value $v_{k+1}$ should be temporarily stored at some places, instead of\n",
    "\n",
    "In step 2, the best policy is the one that takes the action with maximal expected return given a state:\n",
    "\n",
    "$a = {argmax}_a E_{s'}[r(s, a) + \\gamma v_{k}(s')]$\n",
    "\n",
    "Policy iteration algorithm has an outer loop (update policy, step 1 to 3) and an inner loop (fit the value function, within step 1). \n",
    "\n",
    "In each outer loop, we call once `trainer.train()`, where we call `trainer.update_value_function()` once to update the value function (the state value table). \n",
    "\n",
    "After that we call `trainer.update_policy()` to update the current policy. \n",
    "\n",
    "`trainer` object has a `trainer.policy` attribute, which is a function that takes observation as input and returns an action.\n",
    "\n",
    "You should implement the trainer following the framework we already wrote for you. Please carefully go through the codes and finish all `TODO` in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "import random\n",
    "\n",
    "class PolicyIterationTrainer(TabularRLTrainerAbstract):\n",
    "    def __init__(self, gamma=1.0, eps=1e-10, env_name='FrozenLake8x8-v1'):\n",
    "        super(PolicyIterationTrainer, self).__init__(env_name)\n",
    "\n",
    "        # Discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Value function convergence criterion\n",
    "        self.eps = eps\n",
    "\n",
    "        # The **value table** for each possible observation\n",
    "        self.value_table: np.ndarray = np.zeros((self.obs_dim,))\n",
    "\n",
    "        # TODO: you need to implement a uniform random policy at the beginning.\n",
    "        # self.policy is a python function that takes an integer (the observation)\n",
    "        # as input and return an integer (action).\n",
    "        # You can use self.action_dim to get the dimension (range)\n",
    "        # of the action. An action is an integer in range\n",
    "        # [0, ..., self.action_dim - 1]\n",
    "        # Note: policy should be a deterministic function. That is, given a state,\n",
    "        # it should also return the same action.\n",
    "        random_policy = np.array([random.randint(0, self.action_dim - 1) for _ in range(self.obs_dim)])\n",
    "\n",
    "        self.policy: Callable = lambda observation: random_policy[observation]\n",
    "\n",
    "        # test your random policy\n",
    "        test_random_policy(self.policy, self.env)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # TODO: self.value_table may be need to be reset to zeros.\n",
    "        # If you think it should, than do it. If not, then go ahead.\n",
    "        self.value_table: np.ndarray = np.zeros((self.obs_dim,))\n",
    "\n",
    "        self.update_value_function()\n",
    "        self.update_policy()\n",
    "\n",
    "    def update_value_function(self):\n",
    "        count = 0  # count the steps of value updates\n",
    "        while True:\n",
    "            old_table = self.value_table.copy()\n",
    "            delta = 0\n",
    "            for state in range(self.obs_dim):\n",
    "                action = self.policy(state)\n",
    "                transition_list = self._get_transitions(state, action)\n",
    "\n",
    "                state_value = 0\n",
    "                # Iterate over all possible next states given a state-action pair.\n",
    "                for transition in transition_list:\n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "\n",
    "                    # TODO: compute state_value\n",
    "                    # hint: you should use reward, self.gamma, old_table, prob,\n",
    "                    # and next_state to compute the state value\n",
    "                    state_value += prob * (reward + (not done)*(self.gamma*old_table[next_state]))\n",
    "\n",
    "                # update the state value\n",
    "                self.value_table[state] = state_value\n",
    "\n",
    "                delta = max(delta, abs(old_table[state] - state_value))\n",
    "\n",
    "            # TODO: Compare the old_table and current table to\n",
    "            #  decide whether to break the value update process.\n",
    "            # hint: you should use self.eps, old_table and self.value_table\n",
    "            should_break: bool = delta < self.eps\n",
    "            pass\n",
    "\n",
    "            if should_break:\n",
    "                print(\"[DEBUG]\\tThe value table was updated for {} steps. \"\n",
    "                      \"Difference between new and old table is: {:.4f}\".format(\n",
    "                    count, np.sum(np.abs(old_table - self.value_table))\n",
    "                ))\n",
    "                break\n",
    "            count += 1\n",
    "            if count > 6000:\n",
    "                raise ValueError(\"Clearly your code has problem. Check it!\")\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"You need to define a new policy function, given current\n",
    "        value function. The best action for a given state is the one that\n",
    "        has the highest expected return.\n",
    "\n",
    "        To optimize computing efficiency, we introduce a policy table,\n",
    "        which is a numpy array taking state as index and return the action given a state.\n",
    "        \"\"\"\n",
    "        policy_table: np.ndarray = np.zeros([self.obs_dim, ], dtype=int)\n",
    "\n",
    "        for state in range(self.obs_dim):\n",
    "            state_action_values = [0] * self.action_dim\n",
    "\n",
    "            # TODO: assign the action with greatest state-action value\n",
    "            # to policy_table[state].\n",
    "            # Hint:\n",
    "            #  You should use the value table, gamma, reward, as well as \n",
    "            #  the return from self._get_transitions() to compute the\n",
    "            #  state-action value first before getting the action.\n",
    "            #  Bellman equation may help.\n",
    "            best_action = None\n",
    "\n",
    "            for action in range(self.action_dim): \n",
    "                action_value = 0\n",
    "                transition_list = self._get_transitions(state, action)\n",
    "\n",
    "                for transition in transition_list: \n",
    "                    prob = transition['prob']\n",
    "                    reward = transition['reward']\n",
    "                    next_state = transition['next_state']\n",
    "                    done = transition['done']\n",
    "\n",
    "                    action_value += prob * (reward + self.gamma * self.value_table[next_state] * (not done))\n",
    "\n",
    "                state_action_values[action] = action_value\n",
    "\n",
    "            best_action = np.argmax(state_action_values)\n",
    "            policy_table[state] = best_action\n",
    "\n",
    "        self.policy = lambda obs: policy_table[obs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have built the Trainer class for policy iteration algorithm. In the following few cells, we will train the agent to solve the problem and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_pi_config = dict(\n",
    "    max_iteration=1000,\n",
    "    evaluate_interval=1,\n",
    "    gamma=1.0,\n",
    "    eps=1e-10\n",
    ")\n",
    "\n",
    "\n",
    "def policy_iteration(train_config=None):\n",
    "    \n",
    "    # Prepare a config dict\n",
    "    config = default_pi_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    # Initialize the trainer\n",
    "    trainer = PolicyIterationTrainer(gamma=config['gamma'], eps=config['eps'])\n",
    "\n",
    "    # Initialize an array as the policy mapping obs to action.\n",
    "    old_policy = np.zeros(trainer.obs_dim, dtype=int)\n",
    "    old_policy.fill(-1)\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "\n",
    "        # train the agent\n",
    "        trainer.train()\n",
    "\n",
    "        # TODO: compare the new policy with old policy to check whether\n",
    "        #  we should stop. If new and old policy have same output given any\n",
    "        #  observation, then we consider the algorithm is converged and\n",
    "        #  should be stopped.\n",
    "\n",
    "        new_policy = np.zeros(trainer.obs_dim, dtype=int)\n",
    "        for state in range(trainer.obs_dim):\n",
    "            new_policy[state] = trainer.policy(state)\n",
    "\n",
    "        should_stop: bool = np.array_equal(old_policy, new_policy)\n",
    "\n",
    "        if should_stop:\n",
    "            print(\"We found policy is not changed anymore at \"\n",
    "                  \"iteration {}. Current mean episode reward \"\n",
    "                  \"is {}. Stop training.\".format(i, trainer.evaluate()))\n",
    "            break\n",
    "        old_policy = new_policy\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\n",
    "                \"[INFO]\\tAfter {} iterations, current policy has mean episode reward {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "\n",
    "            if i > 20:\n",
    "                print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "                      \"({}) iterations to train a policy iteration \"\n",
    "                      \"agent.\".format(i))\n",
    "\n",
    "    assert trainer.evaluate() > 0.8, \\\n",
    "        \"We expect to get the mean episode reward greater than 0.8. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# It may be confusing to call a trainer agent. But that's what we normally do.\n",
    "pi_agent = policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"Your policy iteration agent achieve {} mean episode reward. The optimal score \"\n",
    "      \"should be > 0.8.\".format(pi_agent.evaluate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pi_agent.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pi_agent.print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully implemented the policy iteration trainer (if and only if no error happens at the above cells). \n",
    "\n",
    "Here are few further problems for you to investigate:\n",
    "\n",
    "1. What is the impact of the discount factor gamma?\n",
    "2. What is the impact of the value function convergence criterion epsilon?\n",
    "\n",
    "If you are interested in doing more investigation (not limited to these two), feel free to open new cells at the end of this notebook and left a clear trace of your thinking and coding, which leads to extra credit if you do a good job. It's an optional job, and you can ignore it.\n",
    "\n",
    "Now let's continue our journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Value Iteration\n",
    "\n",
    "Recall the idea of value iteration. We update the state value: \n",
    "\n",
    "$$v_{k+1}(s) = \\max_a E_{s'} [r(s, a) + \\gamma v_{k}(s')]$$\n",
    "\n",
    "wherein the $s'$ is next state, $r$ is the reward, $v_{k}(s')$ is the next state value given by the old (not updated yet) value function. The expectation is computed among all possible transitions (given a state and action pair, it is possible to have many next states, since the environment is not deterministic).\n",
    "\n",
    "The value iteration algorithm does not require an inner loop. It computes the expected return of all possible actions at a given state and uses the maximum of them as the state value. You can imagine it \"pretends\" we already have the optimal policy and run policy iteration based on it. Therefore, we do not need to maintain a policy object in a trainer. We only need to retrieve the optimal policy using the same rule as policy iteration, given current value function.\n",
    "\n",
    "You should implement the trainer following the framework we already wrote for you. Please carefully go through the code and finish all `TODO` in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "\n",
    "class ValueIterationTrainer(PolicyIterationTrainer):\n",
    "    \"\"\"Note that we inherit Policy Iteration Trainer, to reuse the\n",
    "    code of update_policy(). It's same since it get optimal policy from\n",
    "    current state-value table (self.table).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=1.0, env_name='FrozenLake8x8-v1'):\n",
    "        super(ValueIterationTrainer, self).__init__(gamma, None, env_name)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        # TODO: self.value_table may be need to be reset to zeros.\n",
    "        # If you think it should, than do it. If not, then move on.\n",
    "        # pass\n",
    "\n",
    "        # In value iteration, we do not explicit require a\n",
    "        # policy instance to run. We update value function\n",
    "        # directly based on the transitions. Therefore, we\n",
    "        # don't need to run self.update_policy() in each step.\n",
    "        self.update_value_function()\n",
    "\n",
    "    def one_step_lookahead(self, state): \n",
    "        action_values = np.zeros((self.action_dim,))\n",
    "        # TODO: Compute the new state value.\n",
    "        # Hint: try to compute the state-action value first\n",
    "        for action in range(self.action_dim): # iterate through every possible state action pair \n",
    "            transition_list = self._get_transitions(state, action)\n",
    "            # calculate action value \n",
    "            for transition in transition_list: \n",
    "                prob = transition['prob']\n",
    "                reward = transition['reward']\n",
    "                next_state = transition['next_state']\n",
    "                done = transition['done']\n",
    "\n",
    "                action_values[action] += prob * (reward + (not done)*self.gamma*self.value_table[next_state])\n",
    "        \n",
    "        return action_values\n",
    "\n",
    "    def update_value_function(self):\n",
    "        old_table = self.value_table.copy()\n",
    "\n",
    "        for state in range(self.obs_dim):\n",
    "            state_value = 0\n",
    "\n",
    "            # TODO: Compute the new state value.\n",
    "            # Hint: try to compute the state-action value first\n",
    "            action = self.policy(state) # assume this is optimal action \n",
    "            transition_list = self._get_transitions(state, action) # get state-action pair \n",
    "            # Iterate over all possible next states given a state-action pair.\n",
    "            # SUMMATION(p(s', r | s, pi(s)) * (reward + gamma*V(s')))\n",
    "            for transition in transition_list:\n",
    "                prob = transition['prob']\n",
    "                reward = transition['reward']\n",
    "                next_state = transition['next_state']\n",
    "                done = transition['done']\n",
    "\n",
    "                # TODO: compute state_value\n",
    "                # hint: you should use reward, self.gamma, old_table, prob,\n",
    "                # and next_state to compute the state value\n",
    "                # pass\n",
    "\n",
    "                # where V(s') is of the old policy \n",
    "                state_value += prob * (reward + (not done)*(self.gamma*old_table[next_state])) \n",
    "\n",
    "            self.value_table[state] = state_value\n",
    "\n",
    "        # Till now the one-step value update is finished.\n",
    "        # You can see that we do not use an inner loop to update\n",
    "        # the value function like what we did in the policy iteration.\n",
    "        # This is because to compute the state value, which is\n",
    "        # an expectation among all possible action given by a\n",
    "        # specified policy, we **pretend** we already have the optimal\n",
    "        # policy (the max operation). Therefore we don't need to \n",
    "        # compute the state-action values for those actions that will not\n",
    "        # be selected by the policy.\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Since in value iteration we do not maintain a policy function,\n",
    "        so we need to retrieve it when we need it.\"\"\"\n",
    "        self.update_policy()\n",
    "        return super().evaluate()\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Since in value iteration we do not maintain a policy function,\n",
    "        so we need to retrieve it when we need it.\"\"\"\n",
    "        self.update_policy()\n",
    "        return super().render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_vi_config = dict(\n",
    "    max_iteration=10000,\n",
    "    evaluate_interval=100,  # don't need to update policy each iteration\n",
    "    gamma=1.0,\n",
    "    eps=1e-10\n",
    ")\n",
    "\n",
    "\n",
    "def value_iteration(train_config=None):\n",
    "    config = default_vi_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    # TODO: initialize Value Iteration Trainer. Remember to pass\n",
    "    #  config['gamma'] to it.\n",
    "    trainer: TabularRLTrainerAbstract = ValueIterationTrainer(gamma=config['gamma'])\n",
    "\n",
    "    # old_state_value_table = trainer.value_table.copy()\n",
    "    old_policy = np.zeros(trainer.obs_dim, dtype=int)\n",
    "    old_policy.fill(-1)\n",
    "\n",
    "    for i in range(config['max_iteration']):\n",
    "        # train the agent\n",
    "        trainer.train()\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            print(\"[INFO]\\tIn {} iteration, current \"\n",
    "                  \"mean episode reward is {}.\".format(\n",
    "                i, trainer.evaluate()\n",
    "            ))\n",
    "\n",
    "            # TODO: Compare the new policy with old policy to check should\n",
    "            #  we stop.\n",
    "            # Hint: If new and old policy have same output given any\n",
    "            #  observation, them we consider the algorithm is converged and\n",
    "            #  should be stopped.\n",
    "            \n",
    "            new_policy = np.zeros(trainer.obs_dim, dtype=int)\n",
    "            for state in range(trainer.obs_dim):\n",
    "                new_policy[state] = trainer.policy(state)\n",
    "\n",
    "            should_stop: bool = np.array_equal(old_policy, new_policy)\n",
    "\n",
    "            if should_stop:\n",
    "                print(\"We found policy is not changed anymore at \"\n",
    "                      \"iteration {}. Current mean episode reward \"\n",
    "                      \"is {}. Stop training.\".format(i, trainer.evaluate()))\n",
    "                break\n",
    "\n",
    "            old_policy = new_policy\n",
    "\n",
    "            if i > 3000:\n",
    "                print(\"You sure your codes is OK? It shouldn't take so many \"\n",
    "                      \"({}) iterations to train a policy iteration \"\n",
    "                      \"agent.\".format(\n",
    "                    i))\n",
    "\n",
    "    assert trainer.evaluate() > 0.8, \\\n",
    "        \"We expect to get the mean episode reward greater than 0.8. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "vi_agent = value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "print(\"Your value iteration agent achieve {} mean episode reward. The optimal score \"\n",
    "      \"should be > 0.8.\".format(vi_agent.evaluate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "vi_agent.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "vi_agent.print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation! You have successfully implemented the value iteration trainer (if and only if no error happens at the above cells). Few further problems for you to investigate:\n",
    "\n",
    "1. Do you see that some iteration during training yields better rewards than the final one?  Why does that happen?\n",
    "2. What is the impact of the discount factor gamma?\n",
    "3. What is the impact of the value function convergence criterion epsilon?\n",
    "\n",
    "If you are interested in doing more investigation (not limited to these two), feel free to open new cells at the end of this notebook and left a clear trace of your thinking and coding, which leads to extra credit if you do a good job. It's an optional job, and you can ignore it.\n",
    "\n",
    "Now let's continue our journey!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3: Compare two model-based agents\n",
    "\n",
    "Now we have two agents: `pi_agent` and `vi_agent`. They are believed to be the optimal policies in this environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODO and remove `pass`\n",
    "\n",
    "# TODO: Print the value tables of these two policies and see if they match each other.\n",
    "print(f\"pi_agent: \")\n",
    "pi_agent.print_table()\n",
    "\n",
    "print(f\"vi_agent: \")\n",
    "vi_agent.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can do more investigation here if you wish. Leave it blank if you don't.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Conclusion and Discussion\n",
    "\n",
    "In this assignment, we learn how to use the gym (now Gymnasium) library, how to use Object Oriented Programming to build a basic tabular RL algorithm.\n",
    "\n",
    "Follow the submission instruction in the README to submit your assignment. Thank you!\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. What is the impact of the discount factor gamma?\n",
    "\n",
    "- Timeframe: a higher gamma values more long-term rewards while a lower gamma values immediate rewards more. \n",
    "- Randomness and Exploration: higher gamma allows for more cautious exploration while a lower value allows for more exploration without concern for long-term rewards/consequences.\n",
    "- Speed: a higher gamma can slow down the convergence speed because it values long-term rewards - so it will take more time to explore.  \n",
    "\n",
    "##### 2. What is the impact of the value function convergence criterion epsilon?\n",
    "\n",
    "- Precision: a higher epsilon allows for more precision in optimal values. a lower epsilon can increase precision because it ensures the delta/error is smaller. \n",
    "- Time: a higher epsilon allows for it to converge faster - but it can cause higher inaccuracy.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
